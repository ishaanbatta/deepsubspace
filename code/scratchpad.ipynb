{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import nibabel as nib\n",
    "from scipy.io import loadmat\n",
    "from importlib import reload\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pdb\n",
    "import argparse\n",
    "import json \n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy.stats import ttest_1samp, ttest_ind, zscore\n",
    "from statsmodels.stats.multitest import fdrcorrection, multipletests\n",
    "from mne.stats import fdr_correction\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "from mne import io\n",
    "from mne.datasets import sample\n",
    "from mne.stats import bonferroni_correction, fdr_correction\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Atlases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Harvard-Oxford atlases for cortical and sub-cortical regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/sysapps/ubuntu-applications/fsl/6.0.5.2/fsl/data/atlases/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subcort = nib.load(basepath + 'HarvardOxford/HarvardOxford-sub-maxprob-thr25-1mm.nii.gz')\n",
    "subcort_data = np.array(subcort.get_fdata(), dtype=int)\n",
    "subcort_names = pd.read_csv('../../../data/atlases/HO-Subcort-ROIs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cort = nib.load(basepath + 'HarvardOxford/HarvardOxford-cort-maxprob-thr25-1mm.nii.gz')\n",
    "cort_names = pd.read_csv('../../../data/atlases/HO-Cort-ROIs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = nib.load(basepath + 'Cerebellum/Cerebellum-MNIflirt-maxprob-thr25-1mm.nii.gz')\n",
    "cb_data = np.array(cb.get_fdata(), dtype=int)\n",
    "cb_names = pd.read_csv('../../../data/atlases/HO-CB-ROIs.csv')\n",
    "# cb_data[cb_data>0] = 1 # Binarize, as CB is needed as one region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_names = pd.DataFrame([],columns=cort_names.columns)\n",
    "out_names = out_names.append(cort_names, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = np.array(cort.get_fdata(), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120948  78196  70031  55269  45499  45246  44787  41973  37827  32849\n",
      "  26880  25157  23450  23252  21879  21449  20844  19542  19228  18720\n",
      "  16041  15745  14953  14538  13831  12680  11973  11954  11759  11674\n",
      "  11317   9956   9934   9503   9021   8739   8032   7796   6952   6488\n",
      "   5871   5698   5385   5313   4901   4730   4482   2081]\n"
     ]
    }
   ],
   "source": [
    "c1 = np.array([[i,(out_data==i).sum()] for i in np.unique(out_data) if i!=0 ])\n",
    "print(-np.sort(-c1[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512675 509101 250063 249575  38610  10385  10211   9299   8283   6397\n",
      "   6397   5688   5575   4127   3949   2848   2476   2133   2118    756\n",
      "    666]\n"
     ]
    }
   ],
   "source": [
    "c2 = np.array([[i,(subcort_data==i).sum()] for i in np.unique(subcort_data) if i!=0])\n",
    "print(-np.sort(-c2[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21097 20988 15898 15207 13113 11958  6765  6761  6571  5970  5784  5695\n",
      "  5677  5670  5630  5598  5221  5170  2975  1779   984   954   926   658\n",
      "   580   367   113]\n"
     ]
    }
   ],
   "source": [
    "c3 = np.array([[i,(cb_data==i).sum()] for i in np.unique(cb_data) if i!=0])\n",
    "print(-np.sort(-c3[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178109\n"
     ]
    }
   ],
   "source": [
    "print(-np.sort(-c3[:,1]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(subcort_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 0\n",
      "Skipping 1\n",
      "Skipping 11\n",
      "Skipping 12\n"
     ]
    }
   ],
   "source": [
    "# Replace label numbers for subcortical data to append to cortical\n",
    "offset = 48 # There are 0-47 labels in cortical regions\n",
    "remove_indices = np.array([0,1,11,12])\n",
    "for li in range(21):\n",
    "    if li in remove_indices:\n",
    "        print('Skipping %d'%li)\n",
    "        continue\n",
    "    out_data[subcort_data==(li+1)] = li+1+offset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 52,\n",
       "       53, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49,Left Cerebral White Matter will be removed\n",
      "50,Left Cerebral Cortex  will be removed\n",
      "Replacing 49,Left Cerebral White Matter with 51,Left Lateral Ventricle\n",
      "Replacing 50,Left Cerebral Cortex  with 52,Left Thalamus\n",
      "Replacing 51,Left Lateral Ventricle with 53,Left Caudate\n",
      "Replacing 52,Left Thalamus with 54,Left Putamen\n",
      "Replacing 53,Left Caudate with 55,Left Pallidum\n",
      "Replacing 54,Left Putamen with 56,Brain-Stem\n",
      "Replacing 55,Left Pallidum with 57,Left Hippocampus\n",
      "Replacing 56,Brain-Stem with 58,Left Amygdala\n",
      "Replacing 57,Left Hippocampus with 59,Left Accumbens\n",
      "60,Right Cerebral White Matter will be removed\n",
      "61,Right Cerebral Cortex  will be removed\n",
      "Replacing 58,Left Amygdala with 62,Right Lateral Ventricle\n",
      "Replacing 59,Left Accumbens with 63,Right Thalamus\n",
      "Replacing 60,Right Cerebral White Matter with 64,Right Caudate\n",
      "Replacing 61,Right Cerebral Cortex  with 65,Right Putamen\n",
      "Replacing 62,Right Lateral Ventricle with 66,Right Pallidum\n",
      "Replacing 63,Right Thalamus with 67,Right Hippocampus\n",
      "Replacing 64,Right Caudate with 68,Right Amygdala\n",
      "Replacing 65,Right Putamen with 69,Right Accumbens\n"
     ]
    }
   ],
   "source": [
    "# Remove cortical and white matter voxels from subcortical map\n",
    "remove_values = remove_indices + 1 + offset\n",
    "# remove_voxels = np.zeros(subcort_data.shape, dtype=bool)\n",
    "adder = 0\n",
    "new_map = {}\n",
    "for v in range(1+offset,70):\n",
    "    if v in remove_values:\n",
    "        adder += 1\n",
    "        out_data[out_data==v] = 0\n",
    "        print('%d,%s will be removed'%(v, subcort_names['name'][v-offset-1]))\n",
    "    else:\n",
    "        print('Replacing %d,%s with %d,%s'%\n",
    "              (v-adder,subcort_names['name'][v-offset-1-adder], v,subcort_names['name'][v-offset-1]))\n",
    "        out_data[out_data==v] = v - adder\n",
    "        names_row = {ci:subcort_names.iloc[v-offset-1][ci] for ci in subcort_names.columns}\n",
    "        names_row['index'] = v - adder - 1\n",
    "        out_names = out_names.append(names_row, ignore_index=True)\n",
    "        new_map[v-1] = v-1 - adder  # -1 because of indices\n",
    "    # remove_voxels = np.logical_or(remove_voxels, out_data == rv)\n",
    "# out_data[np.where(remove_voxels==True)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "65 in out_names['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Cerbellar Regions with new indices for some of the combined regions.\n",
    "cboffset = len(out_names)\n",
    "new_cb_map = {}\n",
    "for i in range(27):\n",
    "    li = cb_names.iloc[i]['newindex']\n",
    "    out_data[cb_data == i+1] = li+1 + cboffset\n",
    "    \n",
    "    if not li+cboffset in out_names['index']:\n",
    "        ignore_cols = ['newindex','newname','newshortname']\n",
    "        names_row = {ci:cb_names.iloc[i][ci] for ci in cb_names.columns if ci not in ignore_cols}\n",
    "        names_row['index'] = cb_names.iloc[i]['newindex'] + cboffset\n",
    "        names_row['shortname'] = cb_names.iloc[i]['newshortname']\n",
    "        names_row['name'] = cb_names.iloc[i]['newname']    \n",
    "        out_names = out_names.append(names_row, ignore_index=True)\n",
    "    new_cb_map[i+offset] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '../../../data/atlases/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.save(nib.Nifti1Image(np.array(out_data, dtype=int), affine=cort.affine, header=cort.header), outpath+'HO-CortSubcortCB-atlas.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_names.to_csv(outpath+'HO-CortSubcortCB-ROIs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outpath + 'new_subcort_roi_map.json','w+') as f:\n",
    "    json.dump(new_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outpath + 'new_CB_roi_map.json','w+') as f:\n",
    "    json.dump(new_cb_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(out_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Mask for each ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx,ny,nz = out_data.shape\n",
    "nm = np.unique(out_data).max()\n",
    "out_masks = []\n",
    "\n",
    "for i in range(nm):\n",
    "    curmask = np.zeros(out_data.shape, dtype=int)\n",
    "    curmask[out_data==(i+1)] = i+1\n",
    "    out_masks.append(curmask)\n",
    "out_masks = np.array(out_masks,dtype=int)\n",
    "out_masks = np.moveaxis(out_masks, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.save(nib.Nifti1Image(np.array(out_masks, dtype=int), affine=cort.affine, header=cort.header), outpath+'HO-CortSubcortCB-atlas-ROImasks.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as ut \n",
    "import summarize as smr\n",
    "from models import AN3Ddr_lowresMax\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nib.load('/data/users2/ibatta/data/features/lowresSMRI/ADNI/random_subject.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros(a.get_fdata().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.save(nib.Nifti1Image(b, affine=a.affine, header=a.header), 'check_nibsave.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "saltype = 'fsal_raw'\n",
    "basedir = '../out/results/latest/'\n",
    "config_string = 'mt_AN3DdrlrMx_fkey_lT1_scorename_xyz_iter_*_nc_2_rep_*_bs_32_lr_0.0001_espat_20' \n",
    "configlist_file = '../in/config_keys/allcombos_baseline_clf_nc2'\n",
    "files_key = basedir + config_string + '/' + saltype + '.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import summarize as smr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt_AN3DdrlrMx_fkey_lT1_scorename_age_iter_*_nc_2_rep_*_bs_32_lr_0.0001_espat_20\n",
      "fsal_raw\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ef78e325e5f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize_all_saliencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiglist_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels_3way'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/users2/ibatta/projects/deepsubspace/code/summarize.py\u001b[0m in \u001b[0;36msummarize_all_saliencies\u001b[0;34m(configlist_file, regvar, cfg_ind, rep)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0msummarize_saliency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaltype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaltype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'single'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0msummarize_saliency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaltype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaltype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'groupwise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/users2/ibatta/projects/deepsubspace/code/summarize.py\u001b[0m in \u001b[0;36msummarize_saliency\u001b[0;34m(config_string, regvar, outfile, saltype, mode, rep)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mtstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_tstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malternateH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'two-sided'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'groupwise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mtstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_tstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malternateH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'two-sided'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/users2/ibatta/projects/deepsubspace/code/summarize.py\u001b[0m in \u001b[0;36mrun_tstats\u001b[0;34m(data, data2, alternateH, standardize)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "smr.summarize_all_saliencies(configlist_file, 'labels_3way',10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smr.summarize_all_saliencies(configlist_file, 'labels_3way',10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = '../out/saliency_stats/' + config_string.replace('*','X') + '/tstats.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smr.summarize_saliency(config_string, 'labels_3way',outfile, saltype=saltype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgpath = glob.glob(basedir + config_string)[0] + '/config.pkl' \n",
    "cfg = ut.loadCfg(cfgpath)\n",
    "masks = ut.loadMasks(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_masked_saliency_maps(config_string, masks, saltype, normalize=True):\n",
    "    \"\"\"\n",
    "    Loads saliency maps from all the files in the mentioned config string into a single list of length nch\n",
    "\n",
    "    Args:\n",
    "        config_string (str): keywords to be used as configuration string to search from models outputs directory\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    basedir = '../out/results/latest/'\n",
    "    \n",
    "    nch, nx, ny, nz = masks.shape\n",
    "    mr = masks.reshape([nch,nx*ny*nz])\n",
    "    \n",
    "    out_data = [[] for ch in range(nch)]\n",
    "    for salfile in glob.glob(basedir+config_string+'/'+saltype+'.pkl'):\n",
    "        with open (salfile, 'rb') as f:\n",
    "            sal_data = pickle.load(f)\n",
    "        assert np.all(sal_data.shape[1:] == masks.shape)\n",
    "        nsub, _, _, _, _ = sal_data.shape\n",
    "        sal_data = sal_data.reshape([nsub,nch,nx*ny*nz])\n",
    "        for ch in range(nch):\n",
    "            out_data[ch].append(sal_data[:,ch,mr[ch]==1])\n",
    "    for ch in range(nch):\n",
    "        out_data[ch] = np.vstack(out_data[ch])\n",
    "        if normalize:\n",
    "            out_data[ch] = ut.normalize_image(out_data[ch], method='zscore', axis=1)\n",
    "        \n",
    "    return out_data        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = load_masked_saliency_maps(config_string, masks, saltype, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdc = [md[i][0] for i in range(len(md))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdc[0].shape , mdc[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdc = [ut.normalize_image(mdc[i]) for i in range(len(mdc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n",
    "              [ 0.7149,  0.0775,  0.6072,  0.9656],\n",
    "              [ 0.6341,  0.1403,  0.9759,  0.4064],\n",
    "              [ 0.5918,  0.6948,  0.904 ,  0.3721],\n",
    "              [ 0.0921,  0.2481,  0.1188,  0.1366]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore(b, axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([b,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore(c, axis=2).mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_masked_maps(data, masks, outfilepath):\n",
    "    # mask is a 4-D np array with shape (nch, nx, ny, nz)\n",
    "    # data is a list (and not np.array!) of vectors, each with different length depending on non-zero voxels of the corresponding mask\n",
    "    # outfilepath should be a path with .nii or .nii.gz as extension.\n",
    "    \n",
    "    nch, nx, ny, nz = masks.shape\n",
    "    assert len(data) == nch\n",
    "    mr = masks.reshape(nch, nx*ny*nz)\n",
    "    \n",
    "    cur_out = np.zeros([nch, nx*ny*nz])\n",
    "    for ch in range(nch): \n",
    "        cur_out[ch,mr[ch]==1] = data[ch]\n",
    "    \n",
    "    cur_out =  np.moveaxis(cur_out.reshape([nch,nx,ny,nz]), 0, -1)\n",
    "    nib.save(nib.Nifti1Image(cur_out, np.eye(4)), outfilepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_masked_maps(mdc, masks, '../out/check_save.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskfile = \"/data/users2/ibatta/data/masks/ADNI/ADNI_T1_lowres.nii.gz\"\n",
    "mask = nib.load(maskfile).get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = mask.reshape(np.prod(mask.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salfile = glob.glob(basedir+config_string+'/'+saltype+'.pkl')[0]\n",
    "with open(salfile, 'rb') as f:\n",
    "    sal_data = pickle.load(f)\n",
    "sal_data = ut.normalize_5D(np.abs(sal_data), method='zscore')\n",
    "n, nch, nx, ny, nz = sal_data.shape\n",
    "print(sal_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore(sal_data[0,0], axis=None).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_data[0,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = sal_data.reshape([n, nch, nx*ny*nz])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd[:,mr==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p = ttest_1samp((sd[:,mr==1]), 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_data[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape, p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nch*nx*ny*nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.min(), p.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out  = multipletests(p.reshape([nch*nx*ny*nz]), method='fdr_bh')\n",
    "out  = multipletests(p, method='fdr_bh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = plt.hist(pc,bins=100, density=True, cumulative=False, histtype='stepfilled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (rej, pc) = fdrcorrection(p.reshape([nch*nx*ny*nz]))\n",
    "(rej, pc) = fdrcorrection(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros([nx*ny*nz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[mr==1] = pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.reshape([1,nx,ny,nz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nib.save(nib.Nifti1Image(a, np.eye(4)), \n",
    "             'check.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.min(), pc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(sal_data), np.min(sal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rej, pc) = fdr_correction(p.reshape([nch*nx*ny*nz]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rej = rej.reshape([nch,nx,ny,nz])\n",
    "pc = pc.reshape([nch,nx,ny,nz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc[rej==True][-10:], p[rej==True][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.isnan(p)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rej==False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/data/users2/ibatta/projects/deepsubspace/out/results/mt_AN3DdrlrMx_fkey_lT1_scorename_labels_3way_iter_10_nc_2_rep_0_bs_32_lr_0.0001_espat_20/fimsal.pkl'\n",
    "with open(fp,'rb') as f:\n",
    "    sal = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myminmax(X, axis=0):\n",
    "    return X*np.sum(X)\n",
    "\n",
    "c = np.apply_over_axes(myminmax, a, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal = np.moveaxis(sal,0,-1)\n",
    "nib.save(nib.Nifti1Image(sal[0], np.eye(4)), 'test.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdpath = '../out/modelout/model_state_dict.pt'\n",
    "cfpath = '../out/modelout/config.pkl'\n",
    "\n",
    "# model = AN3Ddr_lowresMax(num_classes=2, num_channels=1, num_groups=1)\n",
    "\n",
    "cfg = ut.loadCfg(cfpath)\n",
    "cfg.ml = '../out/modelout/'\n",
    "model = ut.loadNet(cfg)\n",
    "\n",
    "model.load_state_dict(torch.load(mdpath))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = ut.loadData(cfg, 'te')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, data in enumerate(dataloader, 0):\n",
    "    inputs, labels = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmask = np.ones(inputs.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.cpu()\n",
    "model = model.cpu()\n",
    "ut.run_saliency('../out/modelout/','BP', inputs, model, tempmask, 'labels_3way','clx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = ut.sensitivity_analysis(model, inputs, tempmask, cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = '/data/users2/ibatta/data/features/fmrimeasures/ADNI/nii/002_S_6007/Axial_rsfMRI__Eyes_Open_/2017-03-31_10_40_49.0/S551365/rest/'\n",
    "\n",
    "f1 = nib.load(d+'swarest1_tsavg.nii').get_fdata()\n",
    "f2 = nib.load(d+'swarest1_ALFF.nii').get_fdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(f1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f1-f2 > 10).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = (f1 - f1.min()) / (f1.max()-f1.min())\n",
    "f2 = (f2 - f2.min()) / (f2.max()-f2.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f1-f2 > 0.01).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = '/data/qneuromark/Data/ADNI/Updated/fMRI/ADNI/177_S_6335/Axial_MB_rsfMRI__Eyes_Open_/2020-07-22_12_56_01.0/S951207/rest/swarest1.nii'\n",
    "dl = nib.load(fl).get_fdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir='/data/qneuromark/Data/ADNI/Updated/fMRI/Results/GIGICA/041_S_6401/Axial_rsfMRI__Eyes_Open_/2018-06-12_13_51_03.0/S694594/rest/swarest1/'\n",
    "f1 = 'adni_aa__sub01_component_ica_s1_.nii'\n",
    "f2 = 'adni_aa__ica_c1-1.mat'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = nib.load(basedir + f1).get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = loadmat(basedir + f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2['ic'].shape, d2['tc'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "53*63*52, 66529*194"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  QC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Valued Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ADNI'\n",
    "filemapper = json.load(open('../in/filemapper.json','r'))\n",
    "\n",
    "measures = list(filemapper['filename']['ADNI'].keys())\n",
    "\n",
    "nanfiles = []\n",
    "numnans = []\n",
    "badfiles = []\n",
    "\n",
    "for m in measures: \n",
    "    basepath_mapper = filemapper['basepathmapper'][dataset][m]\n",
    "    basedir = filemapper['basedir'][dataset][basepath_mapper]\n",
    "    # basedir = '/data/qneuromark/Data/ADNI/Updated/fMRI/ADNI/'\n",
    "    filename = filemapper['filename']['ADNI'][m]\n",
    "    # filename = 'swarest1.nii'\n",
    "    for dir in open('../in/%s_MMR180.csv'%basepath_mapper,'r+').read().split('\\n')[:-1]:\n",
    "        fullpath = basedir + dir + filename\n",
    "        scandata = nib.load(fullpath).get_fdata()\n",
    "        if np.isnan(scandata).sum() > 0:\n",
    "            nn = np.isnan(scandata).sum()\n",
    "            print(nn)\n",
    "            print(fullpath)\n",
    "            nanfiles.append(fullpath)\n",
    "            numnans.append(nn)\n",
    "        if np.abs(np.std(scandata)) < 0.001:\n",
    "            print(np.abs(np.std(scandata)))\n",
    "            print(fullpath)\n",
    "            badfiles.append(fullpath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../in/badfiles_measures.txt','w+') as f:\n",
    "    f.write('\\n'.join(badfiles))\n",
    "\n",
    "with open('../in/numnans_measures.txt','w+') as f:\n",
    "    f.write('\\n'.join(['%s,%d'%(nanfiles[i],numnans[i]) for i in range(len(nanfiles))]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(['ALFF', 'DCw', 'DCb', 'fALFF', 'KccReHo', 'VMHC', 'lT1', 'ALFF,DCw,DCb,fALFF,KccReHo,VMHC,lT1', 'ALFF,lT1', 'DCw,lT1', 'DCb,lT1', 'fALFF,lT1', 'KccReHo,lT1', 'VMHC,lT1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../in/vars.txt', 'r+') as f:\n",
    "    scores = f.read().split('\\n')[:-1]\n",
    "scores_file = '../in/analysis_SCORE_MMR180d.csv'\n",
    "df = pd.read_csv(scores_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['age'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for score in scores:\n",
    "    print('%s, %s'%(score,df[score].dtype))\n",
    "    dt = df[score].dtype\n",
    "    if not 'float' in str(dt):\n",
    "        df[score] = df[score].fillna(df[score].mode())\n",
    "    else:\n",
    "        df[score] = df[score].fillna(df[score].median())\n",
    "\n",
    "df.to_csv('../in/analysis_SCORE_MMR180d_imputed.csv',  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../out/performances/baseline/allcombos_baseline_reg.png '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_file = '.'.join(output_file.split('.')[:-1]) + '_pvals.' + output_file.split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = ['tsavg','tsmedian','tsmax','tsmin']\n",
    "    # fns.remove('hT1')\n",
    "    # fns.remove('PerAF')\n",
    "    # fns.append(','.join(fns)) #multimodal with all features into DL model\n",
    "    fns += [fi+',lT1' for fi in fns if 'lT1' not in fi] # multimodal with 2 modalities: with one fmri measure with low-res smri \n",
    "    # fns.append(','.join([','.join(fns) for i in range(8)] )) # Testing 56 groups for future  \n",
    "    print(fns)\n",
    "\n",
    "    # # Map iter value (slurm taskID) to training sample size (tss) and crossvalidation repetition (rep)\n",
    "    fv, rv = np.meshgrid(np.arange(len(fns)), np.arange(cfg.nReps))\n",
    "    fv = fv.reshape((1, np.prod(fv.shape)))\n",
    "    rv = rv.reshape((1, np.prod(rv.shape)))\n",
    "    fkey = fns[fv[0][cfg.iter]]\n",
    "    rep = rv[0][cfg.iter]\n",
    "    print(fkey, rep)\n",
    "    cfg.fkey = fkey\n",
    "    cfg.nch = len(fkey.split(','))\n",
    "    cfg.rep = rep\n",
    "    print(cfg.iter, cfg.tss, cfg.rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = json.load(open('../in/filemapper.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = list(fm['filename']['ADNI'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = ['tsavg','tsmedian','tsmax','tsmin']\n",
    "# fns.remove('hT1')\n",
    "# fns.remove('PerAF')\n",
    "# fns.append(','.join(fns)) #multimodal with all features into DL model\n",
    "fns += [fi+',lT1' for fi in fns if 'lT1' not in fi] # multimodal with 2 modalities: with one fmri measure with low-res smri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "fv, rv, sv = np.meshgrid(np.arange(len(fns)), np.arange(10), np.arange(8))\n",
    "fv = fv.reshape((1, np.prod(fv.shape)))\n",
    "rv = rv.reshape((1, np.prod(rv.shape)))\n",
    "sv = sv.reshape((1, np.prod(sv.shape)))\n",
    "\n",
    "for i in range(80):\n",
    "    fkey = fns[fv[0][i]]\n",
    "    rep = rv[0][i]\n",
    "    scr = sv[0][i]\n",
    "    print(fkey, rep, scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv.shape, rv.shape, sv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "fns.remove('hT1')\n",
    "fns.remove('PerAF')\n",
    "\n",
    "fv, rv = np.meshgrid(np.arange(len(fns)), np.arange(10))\n",
    "fv = fv.reshape((1, np.prod(fv.shape)))\n",
    "rv = rv.reshape((1, np.prod(rv.shape)))\n",
    "\n",
    "for i in range(80):\n",
    "    fkey = fns[fv[0][i]]\n",
    "    rep = rv[0][i]\n",
    "    \n",
    "    print(i,fkey, rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_config(config_string):\n",
    "    \"\"\"\n",
    "    Computes the set of test accuracy for the string corresponding to the config_string. \n",
    "    The config strings should represent one of the directory. \n",
    "    \"\"\"\n",
    "\n",
    "    basedir = '../out/results/latest/'\n",
    "    cmd = 'cat %s%s/test.csv | grep -v acc_te | cut -f2 -d\\\",\\\"'%(basedir,config_string)\n",
    "    output_stream = os.popen(cmd)\n",
    "    test_accs = np.array(output_stream.read().split('\\n')[:-1], dtype=float)\n",
    "    return test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_string = 'AN3Ddr_lowresMax_fkey_ALFF_scorename_labels_3way_iter_*_nc_2_rep_*_bs_32_lr_1e-05_espat_20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = summarize_config(config_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = summarize_config('AN3Ddr_lowresMax_fkey_ALFF_scorename_labels_3way_iter_*_nc_2_rep_*_bs_32_lr_1e-05_espat_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 30\n",
    "k_te = 4\n",
    "k_va = 5\n",
    "nreps = 3\n",
    "\n",
    "all_labels = np.array(list(range(0,30)), dtype=int) // 10\n",
    "\n",
    "# selelct specific labels if needed for binary classification\n",
    "select_labels = [0,2]\n",
    "labels = np.array([li for li in all_labels if li in select_labels], dtype=int)\n",
    "sli = np.array([i for i in range(len(all_labels)) if all_labels[i] in select_labels], dtype=int)\n",
    "\n",
    "\n",
    "for ri in np.arange(nreps):\n",
    "\ttri, test_indices = train_test_split(np.arange(len(labels)), test_size=1.0/k_te, shuffle=True, stratify=labels, random_state=108+ri)\n",
    "\ttr_idx, val_idx = train_test_split(np.arange(len(tri)), test_size=1/k_va, shuffle=True, stratify=labels[tri], random_state=108+ri)\n",
    "\ttrain_indices, val_indices = tri[tr_idx], tri[val_idx]\n",
    "\n",
    "\tprint(all_labels[sli[train_indices]], all_labels[sli[val_indices]], all_labels[sli[test_indices]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt('../out/temp1.txt').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt('../out/temp2.txt').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\tnp.savetxt(outdir+'/tr_r'+str(ri)+'.csv', train_indices, fmt='%d')\n",
    "\tnp.savetxt(outdir+'/va_r'+str(ri)+'.csv', val_indices, fmt='%d')\n",
    "\tnp.savetxt(outdir+'/te_r'+str(ri)+'.csv', test_indices, fmt='%d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr, va = train_test_split(np.arange(n), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Try Merge\n",
    "fl = '../in/lowresfilelist_ADNI_SMRI.txt'\n",
    "sm = '../in/analysis_SCORE_SMRI_fpL.csv'\n",
    "\n",
    "df_fl = pd.read_csv(fl)\n",
    "df_sm = pd.read_csv(sm).iloc[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(12):\n",
    "    tv, rv = np.meshgrid([], np.arange(3))\n",
    "    tv = tv.reshape((1, np.prod(tv.shape)))\n",
    "    rv = rv.reshape((1, np.prod(tv.shape)))\n",
    "    tss = tv[0][iter]\n",
    "    rep = rv[0][iter]\n",
    "    print(tss, rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.loadtxt('../in/SampleSplits/ADNI/3way/te_r0.csv', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm = df_sm.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('AA_DL2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f89dc90b8ce3db6e3c871b8c318edc37b911bdfa52c259173116bc55490ddb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
